---
title: "DATA 621 Assignment 4"
author: "Joby John,Jun Pan"
date: "April 18, 2019"
output:
  word_document: default
  html_document: default
---


```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(car)
library(caTools)
library(caret)
library(corrplot)
library(data.table)
library(dplyr)
library(geoR)
library(ggplot2)
library(grid)
library(gridExtra)
library(kableExtra)
library(knitr)
library(MASS)
library(naniar)
library(nortest)
library(pROC)
library(pscl)
library(psych)
library(reshape)
library(PerformanceAnalytics)
library(ROCR)
library(testthat)
library(UpSetR)
require(leaps)
```

```{r}
train <- read.csv("https://raw.githubusercontent.com/jjohn81/DATA621_Assignment4/master/insurance_training.csv")
test <- read.csv("https://raw.githubusercontent.com/jjohn81/DATA621_Assignment4/master/insurance-evaluation.csv")
```

In this, we will explore, analyze and model a data set containing approximately 8000 records representing a customer at an auto insurance company. Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A "1" means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero. We will build multiple linear regression and binary logistic regression models on the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car.  

#### 1. DATA EXPLORATION (25 Points) 
 
  
Dataset contains `nrow(train)` rows and `ncol(train)` variables. There are 6 missing data in Age, 454 missing data of YOK, 445 missing data of income, 464 missing data of home value. We will use mean of the colums to fill in any missing values. We will also tranform money and other fields formatted numerical fields. 

Summarize dataset into median, quantile, min, max
```{r}
summary(train)
```


Data transformation
```{r}
#remove "$"
money = function(input) {
  out = sub("\\$", "", input)
  out = as.numeric(sub(",", "", out))
  return(out)
}

# Remove " ", replace with "_"
underscore = function(input) {
  out = sub(" ", "_", input)
  return(out)
}

train = as.tbl(train) %>% 
  mutate_at(c("INCOME","HOME_VAL","BLUEBOOK","OLDCLAIM"),
            money) %>% 
  mutate_at(c("EDUCATION","JOB","CAR_TYPE","URBANICITY"),
            underscore) %>% 
  mutate_at(c("EDUCATION","JOB","CAR_TYPE","URBANICITY"),
            as.factor) %>% 
  mutate(TARGET_FLAG = as.factor(TARGET_FLAG))
```


#### 2. DATA PREPARATION
 
Describe how you have transformed the data by changing the original variables or creating new variables. If you did transform the data or create new variables, discuss why you did this. Here are some possible transformations. 
 
a. Fix missing values (maybe with a Mean or Median value) b. Create flags to suggest if a variable was missing c. Transform data by putting it into buckets d. Mathematical transforms such as log or square root (or use Box-Cox) e. Combine variables (such as ratios or adding or multiplying) to create new variables 


Impute missing data with column median for for income and home value  and  column mean for other columns. Convert indicator variables to 0s and 1s; 1 = Yes, Male for Sex, Commercial for Car Use, Red for RED_CAR, and Highly Urban for URBANICITY
Convert categorical predictor values to indicator variables - EDUCATION, CAR_TYPE, JOB.Transform non normal variables like incone and house values using bobcox tranformation. 
```{r}
train$AGE[is.na(train$AGE)] <- mean(train$AGE, na.rm=TRUE)
train$YOJ[is.na(train$YOJ)] <- mean(train$YOJ, na.rm=TRUE)
train$HOME_VAL[is.na(train$HOME_VAL)] <- median(train$HOME_VAL, na.rm=TRUE)
train$CAR_AGE[is.na(train$CAR_AGE)] <- mean(train$CAR_AGE, na.rm=TRUE)
train$INCOME[is.na(train$INCOME)] <- median(train$INCOME, na.rm=TRUE)
train <- train[complete.cases(train),]
train_clean <- train
train_clean$INDEX <- NULL

```
```{r}
x <- lm(train$TARGET_AMT~train$SEX, data = train)
trainx <-train
trainx$SEX <- ifelse(train$SEX=="M", 0,1)
x1 <-  lm(trainx$TARGET_AMT~trainx$SEX, data = trainx)
summary(x)
summary(x1)
```

INCOME, HOME_VAL, BLUEBOOK, and OLDCLAIM are represented as strings. So we will be extracting the numeric values for these.
```{r}
train_clean$INCOME <- as.numeric(train_clean$INCOME)
train_clean$HOME_VAL <- as.numeric(train_clean$HOME_VAL)
train_clean$BLUEBOOK <- as.numeric(train_clean$BLUEBOOK)
train_clean$OLDCLAIM <- as.numeric(train_clean$OLDCLAIM)
```
 
```{r}
ntrain<-select_if(train_clean, is.numeric)
```


Density plot of variables 
```{r}
#ntrain <- as.data.frame((ntrain))

 par(mfrow=c(3, 3))
#colnames <- dimnames(ntrain)[[2]]

 # for(col in 2:ncol(train)) {

    #d <- density(na.omit(ntrain[,col]))
  # d <- qqnorm(na.omit(train[,col]))
  #  plot(d, type="n", main=colnames[col])
   # polygon(d, col="blue", border="gray")
  #}
  

```
From the density plot, we can find INCOME, HOME_VAL, BLUEBOOK and OLDCLAIM are left skewed.
We are going to do transformation.

```{r}
boxcoxfit(train_clean$INCOME[train_clean$INCOME >0])
 
train_clean$INCOME_MOD <- train_clean$INCOME ^0.443
 
boxcoxfit(train_clean$HOME_VAL[train_clean$HOME_VAL > 0])
 
train_clean$HOME_VAL_MOD <- train_clean$HOME_VAL^0.102
 
boxcoxfit(train_clean$BLUEBOOK)
 
train_clean$BLUEBOOK_MOD <- train_clean$BLUEBOOK^0.461
 
boxcoxfit(train_clean$OLDCLAIM[train_clean$OLDCLAIM>0])
 
train_clean$OLD_CLAIM_MOD <- log(train_clean$OLDCLAIM + 1)   
 
```

```{r}
pairs(~MVR_PTS+CLM_FREQ+URBANICITY+HOME_VAL+PARENT1+CAR_USE+OLDCLAIM, data=train_clean, main="Predictors with High Correlattions to Targets", col="slategrey")
```





3. BUILD MODELS (25 Points) 
 
Using the training data set, build at least two different multiple linear regression models and three different binary logistic regression models, using different variables (or the same variables with different transformations). You may select the variables manually, use an approach such as Forward or Stepwise, use a different approach such as trees, or use a combination of techniques. Describe the techniques you used. If you manually selected a variable for inclusion into the model or exclusion into the model, indicate why this was done. 
 
Discuss the coefficients in the models, do they make sense? For example, if a person has a lot of traffic tickets, you would reasonably expect that person to have more car crashes. If the coefficient is negative (suggesting that the person is a safer driver), then that needs to be discussed. Are you keeping the model even though it is counter intuitive? Why? The boss needs to know. 

```{r}
head(train_clean)
```


#### Models for Accidents

#####First model, we call it original_full_model, to seek the correlation between TARGET_FLAG wiht original viarables.
```{r}

train_flag <- train[,-c(1)] 
full.model.glm <- glm(TARGET_FLAG ~.-TARGET_AMT , data = train_flag, family = binomial(link='logit'))
summary(full.model.glm)
```

#####Transform_model: including the transformed variables 
```{r}
#train_flag <- train_clean[,-c(1)] 
transform_model.glm <- glm(TARGET_FLAG ~.-TARGET_AMT, data = train_clean, family = binomial(link='logit'))
summary(transform_model.glm)
```

#####Step Model
```{r}
transform_model_step.glm <- step( transform_model.glm,trace = 0, keep = NULL)
summary(transform_model_step.glm) 
```



#### Models for Amount
 
#####Full Model 
First model includes all variables excluding index. This model has Adjusted R-squared of 0.2886 and only few variables are statistically significatint.  A model with out 'TARGET_FLAG' has low Adjusted R-squared of 0.06665. Model with 'TARGET_FLAG'  has fewer statistically significatint predictor. Since we are predicting claim amount, we believe we dont need to include 'TARGET_FLAG' and any observation  where 'TARGET_FLAG' is zero. However, model with 'TARGET_FLAG' has Adjusted R-squared value and needs further investigation. Both models are statistically significant comapred to the null model.

```{r}

full.model <- lm(TARGET_AMT ~ .  , data = train_clean, na.action = na.exclude)
summary(full.model)

train1 <- train_clean[ which(train_clean$TARGET_FLAG==1),] 
train1 <- train1[,-c(1)] 

full.model.onlyFlag1 <- lm(TARGET_AMT ~ .  ,data= train1, na.action = na.exclude)

summary(full.model.onlyFlag1)


```

#####Step 
We used Stepwise Algorithm on above models and we got simliar results.  We see from the results below the full model has better Adjusted R-squared and more predictors are statistically significant. 

```{r}
model.step <- step(full.model, trace = 0, keep = NULL)
summary(model.step) 


model.onlyFlga1.step <- step(full.model.onlyFlag1, trace = 0, keep = NULL)
summary(model.onlyFlga1.step) 


```

#####Cross Validation
We see simliar results from this models as well.  We see from the results below the full model has better Adjusted R-squared than model with out TARGET_FLAG.
```{r}
set.seed(123) 
train.control <- trainControl(method = "cv", number = 10)
# Train the model
xModel <- train(TARGET_AMT ~., data = train_clean, method = "lm",
               trControl = train.control)

summary(xModel) 


train1 <- train_clean[ which(train_clean$TARGET_FLAG==1),] 
train1 <- train1[,-c(1)] 
xModel_onlyFlag1 <- train(TARGET_AMT ~., data = train1, method = "lm",
               trControl = train.control)

summary(xModel_onlyFlag1) 

```



 
 
 
 4. SELECT MODELS (25 Points) 
 
##### ROC and AUC
Sensitivity, Specificity ROC and Area under curver are very similar for all the models. We cant select one model over another based on these. 
```{r}
par(mfrow=c(1, 3))
train_flag$predict_full_glm <- ifelse(predict(full.model.glm, train_flag, type='response') > 0.5,1,0)
roc_full_glm <- roc(train_flag$TARGET_FLAG, train_flag$predict_full_glm, plot=T, asp=NA,
                legacy.axes=T, main = "ROC Full Model", col="red", levels=c(0,1))

train_clean$transform_model_glm <- ifelse(predict(transform_model.glm, train_clean, type='response') > 0.5,1,0)

roc_transform_model_glm <- roc(train_clean$TARGET_FLAG, train_clean$transform_model_glm, plot=T, asp=NA,
                legacy.axes=T, main = "ROC Transformed", col="red" ,levels=c(0,1))

train_clean$transform_model_step_glm <-  ifelse(predict(transform_model_step.glm, train_clean, type='response') > 0.5,1,0)

roc_transform_model_step_glm <- roc(train_clean$TARGET_FLAG, train_clean$transform_model_step_glm, plot=T, asp=NA,
                legacy.axes=T, main = "ROC Step Wise", col="red", levels=c(0,1))

```

```{r}
confusionMatrix(table(train_flag$TARGET_FLAG, train_flag$predict_full_glm))$byClass
confusionMatrix(table(train_clean$TARGET_FLAG, train_clean$transform_model_glm))$byClass
confusionMatrix(table(train_clean$TARGET_FLAG, train_clean$transform_model_step_glm))$byClass

auc.model <- matrix(c(auc(roc_full_glm),auc(roc_transform_model_glm), auc(roc_transform_model_step_glm)), ncol = 3, nrow=1, byrow = T)
colnames(auc.model)<-c("Full", "Transformed", "Step Transformed")
rownames(auc.model)<- c("AUC")
auc.model
```


#### AIC 

All three models have very simliar AIC. Transformed and Transformed Step models have slightly better  AIC values. 

```{r  echo= F}
aic <- matrix(c(full.model.glm$aic,transform_model.glm$aic,transform_model.glm$aic), ncol = 3, nrow=1, byrow = T)
colnames(aic)<-c("Full", "Transformed", "Transformed Step")
rownames(aic)<- c("AIC")
aic
```
#### Deviance

Based on the ANOVA summary of the deviance we Transformed and Transformed Step models are slightly better than the full model.

```{r echo= F}
anova(full.model.glm, transform_model.glm, transform_model.glm)

```
##### Models for predicting Target amount

Based on the table below we see that all models with 'TARGET_FLAG1'  have higher Adjusted R-squared values. 
Models with out TARGET_FLAG1 and excluding TARGET_FLAG1=0 observation have low Adjusted R-squared values, therefore doesnt explain the variablity in the data well. RMSE values for the models with TARGET_FLAG1 are also far better than models with out TARGET_FLAG1 suggesting models with TARGET_FLAG1 are better fits the data. 

```{r}

 rsq <-c(summary(full.model)$adj.r.squared,summary(full.model.onlyFlag1)$adj.r.squared,
summary(model.step)$adj.r.squared,summary(model.onlyFlga1.step)$adj.r.squared,
  xModel$results$Rsquared, xModel_onlyFlag1$results$Rsquared)
modelnames <- c('FullModel', 'FullOnlyFlag1', 'FullModelStep','FullOnlyFlag1Step','CrossValidation','CrossValidationFalg1' )
names(rsq)<- modelnames
rsq
rmse <- c(sqrt(mean(full.model$residuals^2)),sqrt(mean(full.model.onlyFlag1$residuals^2)),
sqrt(mean(model.step$residuals^2)),sqrt(mean(model.onlyFlga1.step$residuals^2)),
 xModel$results$RMSE, xModel_onlyFlag1$results$RMSE)

names(rmse)<- modelnames
rmse
par(mfrow=c(2, 3))
plot(full.model$residuals~full.model$fitted.values)
plot(full.model.onlyFlag1$residuals~full.model.onlyFlag1$fitted.values)
plot(model.step$residuals~model.step$fitted.values)
plot(model.onlyFlga1.step$residuals~model.onlyFlga1.step$fitted.values)
plot(xModel$residuals~xModel$fitted.values)
plot(xModel_onlyFlag1$residuals~xModel_onlyFlag1$fitted.values)
```
#Test the reduced model on evaluation data
```{r}
evaluation <- read.csv("https://raw.githubusercontent.com/jjohn81/DATA621_Assignment4/master/insurance-evaluation.csv")
evaluation2 <- evaluation
dim(evaluation)
```
```{r}
str(evaluation)
```






```{r}
evaluation$INDEX <- NULL
evaluation$TARGET_AMT <- 0
evaluation$TARGET_FLAG <- 0
```


```{r}
str(evaluation)
```




```{r}
#remove "$"
money = function(input) {
  out = sub("\\$", "", input)
  out = as.numeric(sub(",", "", out))
  return(out)
}

# Remove " ", replace with "_"
underscore = function(input) {
  out = sub(" ", "_", input)
  return(out)
}

evaluation = as.tbl(evaluation) %>% 
  mutate_at(c("INCOME","HOME_VAL","BLUEBOOK","OLDCLAIM"),
            money) %>% 
  mutate_at(c("EDUCATION","JOB","CAR_TYPE","URBANICITY"),
            underscore) %>% 
  mutate_at(c("EDUCATION","JOB","CAR_TYPE","URBANICITY"),
            as.factor) %>% 
  mutate(TARGET_FLAG = as.factor(TARGET_FLAG))
```

```{r}
evaluation$AGE[is.na(evaluation$AGE)] <- mean(evaluation$AGE, na.rm=TRUE)
evaluation$YOJ[is.na(evaluation$YOJ)] <- mean(evaluation$YOJ, na.rm=TRUE)
evaluation$HOME_VAL[is.na(evaluation$HOME_VAL)] <- mean(evaluation$HOME_VAL, na.rm=TRUE)
evaluation$CAR_AGE[is.na(evaluation$CAR_AGE)] <- mean(evaluation$CAR_AGE, na.rm=TRUE)
evaluation$INCOME[is.na(evaluation$INCOME)] <- mean(evaluation$INCOME, na.rm=TRUE)
evaluation <- evaluation[complete.cases(evaluation),]
```


```{r}
evaluation$INCOME <- as.numeric(evaluation$INCOME)
evaluation$HOME_VAL <- as.numeric(evaluation$HOME_VAL)
evaluation$BLUEBOOK <- as.numeric(evaluation$BLUEBOOK)
evaluation$OLDCLAIM <- as.numeric(evaluation$OLDCLAIM)
```






```{r}
evaluation$INCOME_MOD <- evaluation$INCOME ^0.433
evaluation$HOME_VAL_MOD <- evaluation$HOME_VAL^0.102
evaluation$BLUEBOOK_MOD <- evaluation$BLUEBOOK^0.461
evaluation$OLD_CLAIM_MOD <- log(evaluation$OLDCLAIM + 1) 
```


```{r}
evaluation$PARENT1 <- ifelse(evaluation$PARENT1=="Yes", 1, 0)
evaluation$MSTATUS <- ifelse(evaluation$MSTATUS=="Yes", 1, 0)
evaluation$SEX <- ifelse(evaluation$SEX=="M", 1, 0)
evaluation$CAR_USE <- ifelse(evaluation$CAR_USE=="Commercial", 1, 0)
evaluation$RED_CAR <- ifelse(evaluation$RED_CAR=="Yes", 1, 0)
evaluation$REVOKED <- ifelse(evaluation$REVOKED=="Yes", 1, 0)
evaluation$URBANICITY <- ifelse(evaluation$URBANICITY == "Highly Urban/ Urban", 1, 0)


evaluation$HSDropout <- ifelse(evaluation$EDUCATION=="<High School", 1, 0)
evaluation$Bachelors <- ifelse(evaluation$EDUCATION=="Bachelors", 1, 0)
evaluation$Masters <- ifelse(evaluation$EDUCATION=="Masters", 1, 0)
evaluation$PhD <- ifelse(evaluation$EDUCATION=="PhD", 1, 0)


evaluation$Panel_Truck <- ifelse(evaluation$CAR_TYPE=="Panel Truck", 1, 0)
evaluation$Pickup <- ifelse(evaluation$CAR_TYPE=="Pickup", 1, 0)
evaluation$Sports_Car <- ifelse(evaluation$CAR_TYPE=="Sports Car", 1, 0)
evaluation$Van <- ifelse(evaluation$CAR_TYPE=="Van", 1, 0)
evaluation$SUV <- ifelse(evaluation$CAR_TYPE=="z_SUV", 1, 0)


evaluation$Professional <- ifelse(evaluation$JOB == "Professional", 1, 0)
evaluation$Blue_Collar <- ifelse(evaluation$JOB == "Professional", 1, 0)
evaluation$Clerical <- ifelse(evaluation$JOB == "Clerical", 1, 0)
evaluation$Doctor <- ifelse(evaluation$JOB == "Doctor", 1, 0)
evaluation$Lawyer <- ifelse(evaluation$JOB == "Lawyer", 1, 0)
evaluation$Manager <- ifelse(evaluation$JOB == "Manager", 1, 0)
evaluation$Home_Maker <- ifelse(evaluation$JOB == "Home Maker", 1, 0)
evaluation$Student <- ifelse(evaluation$JOB == "Student", 1, 0)
```

```{r}
TARGET_FLAG <- predict(reduced_model, newdata = evaluation, type="response")

y_pred_num <- ifelse(TARGET_FLAG > 0.5, 1, 0)
y_pred <- factor(y_pred_num, levels=c(0, 1))
summary(y_pred)
```










